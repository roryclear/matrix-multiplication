{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WfGJQtzuCc6n"
      },
      "outputs": [],
      "source": [
        "import torch, gc\n",
        "import time\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUCI32b-glGH"
      },
      "outputs": [],
      "source": [
        "# strassen CPU attempt\n",
        "\n",
        "a = torch.randn(2,2)\n",
        "b = torch.randn(2,2)\n",
        "\n",
        "# ------\n",
        "\n",
        "def naive(a,b):\n",
        "  n = int(len(a))\n",
        "  c = torch.zeros(n,n)\n",
        "\n",
        "  for y in range(n):\n",
        "    for x in range(n):\n",
        "      for z in range(n):\n",
        "        c[y][x] += a[y][z] * b[z][x]\n",
        "\n",
        "  return c\n",
        "\n",
        "\n",
        "def strassen2x2(a,b):\n",
        "  \n",
        "  m1 = (a[0][0] + a[1][1]) * (b[0][0] + b[1][1])\n",
        "  m2 = (a[1][0] + a[1][1]) * b[0][0]\n",
        "  m3 = a[0][0] * (b[0][1] - b[1][1])\n",
        "  m4 = a[1][1] * (b[1][0] - b[0][0])\n",
        "  m5 = (a[0][0] + a[0][1]) * b[1][1]\n",
        "  m6 = (a[1][0] - a[0][0]) * (b[0][0] + b[0][1])\n",
        "  m7 = (a[0][1] - a[1][1]) * (b[1][0] + b[1][1])\n",
        "  \n",
        "  return torch.tensor([[m1 + m4 - m5 + m7 , m3 + m5],\n",
        "                        [m2 + m4 , m1 - m2 + m3 + m6]])\n",
        "\n",
        "def strassen(a,b):\n",
        "\n",
        "  if a.size() == torch.Size([2, 2]):\n",
        "    return strassen2x2(a,b)\n",
        "\n",
        "  n = int(len(a))\n",
        "  n0 = n\n",
        "  x = math.ceil(math.log2(n))\n",
        "  newSize = int(math.pow(2,x))\n",
        "  if n != newSize:\n",
        "    a2 = torch.zeros(newSize,newSize)\n",
        "    b2 = torch.zeros(newSize,newSize)\n",
        "    a2[:n,:n] = a\n",
        "    b2[:n,:n] = b\n",
        "    a = a2\n",
        "    b = b2\n",
        "\n",
        "  n = int(newSize / 2)\n",
        "\n",
        "  a11 = a[:n,:n]\n",
        "  a12 = a[:n,n:]\n",
        "  a21 = a[n:,:n]\n",
        "  a22 = a[n:,n:]\n",
        "\n",
        "  b11 = b[:n,:n]\n",
        "  b12 = b[:n,n:]\n",
        "  b21 = b[n:,:n]\n",
        "  b22 = b[n:,n:]\n",
        "\n",
        "  m1 = strassen((a11 + a22) , (b11 + b22))\n",
        "  m2 = strassen((a21 + a22) , b11)\n",
        "  m3 = strassen(a11 , (b12 - b22))\n",
        "  m4 = strassen(a22 , (b21 - b11))\n",
        "  m5 = strassen((a11 + a12) , b22)\n",
        "  m6 = strassen((a21 - a11) , (b11 + b12))\n",
        "  m7 = strassen((a12 - a22) , (b21 + b22))\n",
        "\n",
        "  ret = torch.randn(n*2,n*2)\n",
        "  ret[:n,:n] = m1 + m4 - m5 + m7\n",
        "  ret[:n,n:] = m3 + m5\n",
        "  ret[n:,:n] = m2 + m4\n",
        "  ret[n:,n:] = m1 - m2 + m3 + m6\n",
        "\n",
        "  return ret[:n0,:n0]\n",
        "\n",
        "o1 = strassen(a,b)\n",
        "o2 = torch.matmul(a,b)\n",
        "\n",
        "for y in range(len(o1)):\n",
        "  for x in range(len(o1[y])):\n",
        "    assert(abs(o1[y][x].item() - o2[y][x].item()) < 0.000001)\n",
        "\n",
        "a = torch.randn(16,16)\n",
        "b = torch.randn(16,16)\n",
        "\n",
        "o1 = strassen(a,b)\n",
        "o2 = torch.matmul(a,b)\n",
        "o3 = naive(a,b)\n",
        "\n",
        "for y in range(len(o1)):\n",
        "  for x in range(len(o1[y])):\n",
        "    assert abs(o1[y][x].item() - o2[y][x].item()) < 0.0001 , (\"ffs\",o1[y][x].item(),o2[y][x].item())\n",
        "    assert abs(o3[y][x].item() - o2[y][x].item()) < 0.0001 , (\"ffs\",o3[y][x].item(),o2[y][x].item())\n",
        "\n",
        "\n",
        "a = torch.randn(20,20)\n",
        "b = torch.randn(20,20)\n",
        "\n",
        "o1 = strassen(a,b)\n",
        "o2 = torch.matmul(a,b)\n",
        "o3 = naive(a,b)\n",
        "\n",
        "for y in range(len(o1)):\n",
        "  for x in range(len(o1[y])):\n",
        "    assert abs(o1[y][x].item() - o2[y][x].item()) < 0.0001 , (\"ffs\",o1[y][x].item(),o2[y][x].item())\n",
        "    assert abs(o3[y][x].item() - o2[y][x].item()) < 0.0001 , (\"ffs\",o3[y][x].item(),o2[y][x].item())\n",
        "\n",
        "# SPEED TESTS\n",
        "\n",
        "n = 128\n",
        "a = torch.randn(n,n)\n",
        "b = torch.randn(n,n)\n",
        "\n",
        "start_time = time.time()\n",
        "o = torch.matmul(a,b)\n",
        "t = time.time() - start_time\n",
        "print(\"matmul \" + str(n) + \"x\" + str(n) + \" took\",t,\"seconds\")\n",
        "\n",
        "\n",
        "# NAIVE SPEED\n",
        "\n",
        "o = naive(a,b)\n",
        "t1 = time.time() - start_time\n",
        "print(\"naive \" + str(n) + \"x\" + str(n) + \" took\",t1,\"seconds\")\n",
        "\n",
        "n = int(n/2)\n",
        "a = torch.randn(n,n)\n",
        "b = torch.randn(n,n)\n",
        "\n",
        "start_time = time.time()\n",
        "o = naive(a,b)\n",
        "t2 = time.time() - start_time\n",
        "print(\"naive \" + str(n) + \"x\" + str(n) + \" took\",t2,\"seconds\")\n",
        "\n",
        "x = t1 / t2\n",
        "print(x,\"times slower\")\n",
        "\n",
        "p = math.log(x) / math.log(2)\n",
        "\n",
        "print(\"naive CPU -> n^\" + str(p) + \"\\n\\n\")\n",
        "\n",
        "# STRASSEN SPEED\n",
        "\n",
        "n = 128\n",
        "a = torch.randn(n,n)\n",
        "b = torch.randn(n,n)\n",
        "\n",
        "start_time = time.time()\n",
        "o = strassen(a,b)\n",
        "t1 = time.time() - start_time\n",
        "print(\"strassen \" + str(n) + \"x\" + str(n) + \" took\",t1,\"seconds\")\n",
        "\n",
        "n = int(n/2)\n",
        "a = torch.randn(n,n)\n",
        "b = torch.randn(n,n)\n",
        "\n",
        "start_time = time.time()\n",
        "o = strassen(a,b)\n",
        "t2 = time.time() - start_time\n",
        "print(\"strassen \" + str(n) + \"x\" + str(n) + \" took\",t2,\"seconds\")\n",
        "\n",
        "x = t1 / t2\n",
        "print(x,\"times slower\")\n",
        "\n",
        "p = math.log(x) / math.log(2)\n",
        "\n",
        "print(\"strassen CPU -> n^\" + str(p) + \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IG1RX5icCpqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "005e7437-4322-40c2-b020-8cee0137e361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch CPU time 8000x8000 13.162297010421753\n",
            "pytorch CPU time 4000x4000 2.215142250061035\n",
            "5.941964679721623 times slower\n",
            "pytorch CPU -> n^2.5709400295183453\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#pytorch CPU\n",
        "\n",
        "n = 8000\n",
        "m1 = torch.randn(n,n)\n",
        "m2 = torch.randn(n,n)\n",
        "r = torch.randn(n,n)\n",
        "\n",
        "start_time = time.time()\n",
        "r = torch.matmul(m1,m2)\n",
        "\n",
        "t1 = time.time() - start_time\n",
        "print(\"pytorch CPU time \"+str(n)+\"x\"+str(n),t1)\n",
        "\n",
        "n = int(n/2)\n",
        "m1 = torch.randn(n,n)\n",
        "m2 = torch.randn(n,n)\n",
        "r = torch.randn(n,n)\n",
        "\n",
        "start_time = time.time()\n",
        "r = torch.matmul(m1,m2)\n",
        "\n",
        "t2 = time.time() - start_time\n",
        "print(\"pytorch CPU time \"+str(n)+\"x\"+str(n),t2)\n",
        "\n",
        "x = t1 / t2\n",
        "print(x,\"times slower\")\n",
        "\n",
        "p = math.log(x) / math.log(2)\n",
        "\n",
        "print(\"pytorch CPU -> n^\" + str(p) + \"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5C0jtLhlwXy"
      },
      "outputs": [],
      "source": [
        "#pytorch GPU\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "n = 24000\n",
        "m1 = torch.randn(n,n).to(device)\n",
        "m2 = torch.randn(n,n).to(device)\n",
        "r = torch.randn(n,n).to(device)\n",
        "\n",
        "start_time = time.time()\n",
        "r = torch.matmul(m1,m2)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "t1 = time.time() - start_time\n",
        "print(\"pytorch GPU time \"+str(n)+\"x\"+str(n),t1)\n",
        "\n",
        "n = int(n/2)\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "m1 = torch.randn(n,n).to(device)\n",
        "m2 = torch.randn(n,n).to(device)\n",
        "r = torch.randn(n,n).to(device)\n",
        "\n",
        "start_time = time.time()\n",
        "r = torch.matmul(m1,m2)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "t2 = time.time() - start_time\n",
        "print(\"pytorch GPU time \"+str(n)+\"x\"+str(n),t2)\n",
        "\n",
        "x = t1 / t2\n",
        "print(x,\"times slower\")\n",
        "\n",
        "p = math.log(x) / math.log(2)\n",
        "\n",
        "print(\"pytorch GPU -> n^\" + str(p))\n",
        "\n",
        "\n",
        "\n",
        "#running code before this breaks it???\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "n = 1024\n",
        "m1 = torch.randn(n,n).to(device)\n",
        "m2 = torch.randn(n,n).to(device)\n",
        "\n",
        "start_time = time.time()\n",
        "torch.matmul(m1,m2)\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "t = time.time() - start_time\n",
        "print(\"pytorch GPU time \"+str(n)+\"x\"+str(n),t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQQbhdQtxsGG"
      },
      "outputs": [],
      "source": [
        "!pip install pycuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "iBD4JA_ixkWX"
      },
      "outputs": [],
      "source": [
        "# MY OWN CUDA ATTEMPT\n",
        "import pycuda.compiler as comp\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import numpy\n",
        "\n",
        "mod = comp.SourceModule(\n",
        "    \"\"\"\n",
        "  __global__ void matmul(float *ret, float *a, float *b, int length)\n",
        "{\n",
        "  const int index = threadIdx.x + (blockDim.x * blockIdx.x);\n",
        "  const int i = index / length;\n",
        "  const int j = index % length;\n",
        "  if(i < length && j < length)\n",
        "  {\n",
        "    for(int k = 0; k < length; k++)\n",
        "    {\n",
        "      ret[i*length + j] += a[i*length + k] * b[j + k*length];\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "  __global__ void matmul2(float *ret, float *a, float *b, int nca, int ncb, int nra)\n",
        "{\n",
        "  const int index = threadIdx.x + (blockDim.x * blockIdx.x);\n",
        "  const int i = index / nra;\n",
        "  const int j = index % ncb;\n",
        "  if(i < nca && j < ncb)\n",
        "  {\n",
        "    for(int k = 0; k < nca; k++)\n",
        "    {\n",
        "      ret[i*ncb + j] += a[i*nca + k] * b[j + k*ncb];\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        ")\n",
        "numberOfThreads = \\\n",
        "    cuda.Device(0).get_attribute(pycuda._driver.device_attribute.MAX_THREADS_PER_BLOCK)\n",
        "cudaMatmul = mod.get_function(\"matmul\")\n",
        "cudaMatmul2 = mod.get_function(\"matmul2\")\n",
        "\n",
        "\n",
        "m1 = torch.randn(15,20)\n",
        "m2 = torch.randn(20,10)\n",
        "r = torch.zeros(15,10)\n",
        "\n",
        "\n",
        "m1np = m1.numpy()\n",
        "m2np = m2.numpy()\n",
        "rnp = r.numpy()\n",
        "\n",
        "m1cuda = cuda.mem_alloc(m1np.nbytes)\n",
        "m2cuda = cuda.mem_alloc(m2np.nbytes)\n",
        "rcuda = cuda.mem_alloc(rnp.nbytes)\n",
        "\n",
        "cuda.memcpy_htod(m1cuda,m1np)\n",
        "cuda.memcpy_htod(m2cuda,m2np)\n",
        "cuda.memcpy_htod(rcuda,rnp)\n",
        "\n",
        "\n",
        "### check copy works ok\n",
        "cuda.memcpy_dtoh(rnp,rcuda)\n",
        "\n",
        "for y in range(len(rnp)):\n",
        "  for x in range(len(rnp[y])):\n",
        "    assert rnp[y][x] == r[y][x], \"copy didn't work\"\n",
        "\n",
        "### test my matmul answer\n",
        "\n",
        "r = torch.matmul(m1,m2)\n",
        "\n",
        "\n",
        "nca = numpy.int32(len(m1[0]))\n",
        "ncb = numpy.int32(len(m2[0]))\n",
        "nra = numpy.int32(len(m1))\n",
        "\n",
        "ops = nca * ncb * nra\n",
        "bx = numberOfThreads\n",
        "gx = math.ceil(ops / numberOfThreads)\n",
        "\n",
        "\n",
        "#cudaMatmul(rcuda,m1cuda,m2cuda,numpy.int32(n),block=(bx,1,1),grid=(gx,1))\n",
        "cudaMatmul2(rcuda,m1cuda,m2cuda,nca,ncb,nra,block=(bx,1,1),grid=(gx,1))\n",
        "cuda.memcpy_dtoh(rnp,rcuda)\n",
        "\n",
        "for y in range(len(rnp)):\n",
        "  for x in range(len(rnp[y])):\n",
        "    assert abs(rnp[y][x] - r[y][x]) < 0.00001,\"cuda matmul didn't work\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x95w4VBOlsnM"
      },
      "outputs": [],
      "source": [
        "# TEST MY CUDA SPEED\n",
        "\n",
        "n = 1024\n",
        "m1 = torch.randn(n,n)\n",
        "m2 = torch.randn(n,n)\n",
        "r = torch.zeros(n,n)\n",
        "\n",
        "\n",
        "m1np = m1.numpy()\n",
        "m2np = m2.numpy()\n",
        "rnp = r.numpy()\n",
        "\n",
        "m1cuda = cuda.mem_alloc(m1np.nbytes)\n",
        "m2cuda = cuda.mem_alloc(m2np.nbytes)\n",
        "rcuda = cuda.mem_alloc(rnp.nbytes)\n",
        "\n",
        "cuda.memcpy_htod(m1cuda,m1np)\n",
        "cuda.memcpy_htod(m2cuda,m2np)\n",
        "cuda.memcpy_htod(rcuda,rnp)\n",
        "\n",
        "ops = n * n\n",
        "bx = numberOfThreads\n",
        "gx = math.ceil(ops / numberOfThreads)\n",
        "\n",
        "print(\"ops =\",ops)\n",
        "print(\"bx =\",bx)\n",
        "print(\"gx =\",gx)\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "cudaMatmul(rcuda,m1cuda,m2cuda,numpy.int32(n),block=(bx,1,1),grid=(gx,1))\n",
        "cuda.memcpy_dtoh(rnp,rcuda)\n",
        "t = time.time() - start_time\n",
        "print(\"my cuda time \"+str(n)+\"x\"+str(n),t)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}